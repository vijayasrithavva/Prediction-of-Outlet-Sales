# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import pickle

#dataset = pd.read_csv('data.csv')

# -*- coding: utf-8 -*-
"""Sales_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RwOCKq4sKmN0KsYzUe-whEKIg0uMu9Ka
"""


"""**Loading the Dataset**"""

#Load trainSet and use header to known about trainset
df=pd.read_csv('sales.csv')


"""**Preprocessing the dataset**"""

#check for null values
df.isnull().sum()

# check for categorical attributes
cat_col = []
for x in df.dtypes.index:
  if df.dtypes[x] == 'object':
    cat_col.append(x)
cat_col

cat_col.remove('Item_Identifier')
cat_col.remove('Outlet_Identifier')
cat_col

#print the categorical columns
for col in cat_col:
  print(col)
  print(df[col].value_counts())
  print()

#fill the new values
item_weight_mean = df.pivot_table(values = "Item_Weight", index = 'Item_Identifier')
item_weight_mean

df.head()

miss_bool = df['Item_Weight'].isnull()
miss_bool

#import random
#for i,item in enumerate(df['Item_Identifier']):
 # if miss_bool[i]:
  #  if item in item_weight_mean:
   #   df['Item_Weight'][i] = item_weight_mean.loc[item]['Item_weight']
     
    #else:
      #df['Item_Weight'][i] = np.mean(df['Item_Weight'])
     # df['Item_Weight'][i] = df.fillna(method = 'ffill')

df["Item_Weight"].fillna(method = "ffill", inplace = True)

df['Item_Weight'].isnull().sum()

outlet_size_mode = df.pivot_table(values = 'Outlet_Size', columns = 'Outlet_Type', aggfunc = (lambda x: x.mode()[0]))
outlet_size_mode

df.head()

miss_bool = df['Outlet_Size'].isnull()
df.loc[miss_bool, 'Outlet_Size'] = df.loc[miss_bool, 'Outlet_Type'].apply(lambda x: outlet_size_mode[x])

df['Outlet_Size'].isnull().sum()

sum(df['Item_Visibility'] == 0)

# replace zeros with mean
df.loc[:, 'Item_Visibility'].replace([0], [df['Item_Visibility'].mean()], inplace = True)

sum(df['Item_Visibility'] == 0)

# combine item fat content
df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})
df['Item_Fat_Content'].value_counts()

df.head()

"""**Creation Of New Attributes**"""

df['New_Item_Type'] = df['Item_Identifier'].apply(lambda x: x[:2])
df['New_Item_Type']

df['New_Item_Type'] = df['New_Item_Type'].map({'FD':'Food','NC': 'Non-Consumable','DR':'Drinks'})
df['New_Item_Type'].value_counts()

df.loc[df['New_Item_Type'] == 'Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'
df['Item_Fat_Content'].value_counts()

# create small values for establishment year
#df['Outlet_Years'] = 2021 - df['Outlet_Establishment_Year']
#df['Outlet_Years']

df.head()

"""**Exploratory Data Analysis**"""

sns.distplot(df['Item_Weight'])

sns.distplot(df['Item_Visibility'])

sns.distplot(df['Item_MRP'])

sns.distplot(df['Item_Outlet_Sales'])
df['Item_Outlet_Sales'] = np.log(1+df['Item_Outlet_Sales'])

sns.distplot(df['Item_Outlet_Sales'])

# for categorical attributes we use countplot
sns.countplot(df['Item_Fat_Content'])

#plt.figure(figsize= (17,5))
l = list(df['Item_Type'].unique())
chart = sns.countplot(df['Item_Type'])
chart.set_xticklabels(labels=l,rotation = 90)

sns.countplot(df['Outlet_Establishment_Year'])

sns.countplot(df['Outlet_Size'])

sns.countplot(df['Outlet_Location_Type'])

sns.countplot(df['Outlet_Type'])

"""**Corelation Matrix**"""

corr = df.corr()
sns.heatmap(corr,annot = True,cmap = 'coolwarm')



"""**Label Encoding**"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Outlet_Identifier'] = le.fit_transform(df['Outlet_Identifier'])
cat_col = ['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','New_Item_Type','Outlet_Type',]
for col in cat_col:
  df[col] = le.fit_transform(df[col])


"""**Input Split**"""



X = df.drop(columns=['Item_Identifier','Item_Outlet_Sales'])
y = df['Item_Outlet_Sales']

#Splitting Training and Test Set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=10)
from sklearn.ensemble import RandomForestRegressor
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [2,4]
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the param grid
param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(param_grid)
rf_Model = RandomForestRegressor()
from sklearn.model_selection import GridSearchCV
rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)
rf_Grid.fit(X_train, Y_train)
rf_Grid.best_params_

#checking accuracy
print (f'Train Accuracy - : {rf_Grid.score(X_train,Y_train):.3f}')
print (f'Test Accuracy - : {rf_Grid.score(X_test,Y_test):.3f}')




# Saving model to disk
pickle.dump(rf_Grid, open('model.pkl','wb'))

# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))